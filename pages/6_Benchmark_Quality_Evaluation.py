"""Benchmark dataset quality evaluation page."""

import re
from pathlib import Path

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st

from app_utils import ensure_page_config, get_benchmark_info, render_sidebar

# Color palette for quality evaluation
QUALITY_COLORS = {
    'high': '#2ecc71',
    'medium': '#f39c12',
    'low': '#e74c3c',
    'poor': '#e74c3c',
    'moderate': '#f39c12',
    'substantial': '#3498db',
    'almost_perfect': '#2ecc71',
    'gpt4o': '#10a37f',
    'gpt5.1': '#74aa9c',
    'claude_sonnet': '#d97757',
    'claude_haiku': '#f4b088',
    'confidence_low': '#e74c3c',
    'confidence_medium': '#f39c12',
    'confidence_high': '#2ecc71',
}


@st.cache_data
def parse_judge_results_quality(filepath: str) -> dict:
    """Parse judge_llm_result.txt into structured dictionary"""
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()

    data = {}
    lines = content.split('\n')

    for line in lines:
        if 'Total samples:' in line:
            data['total_samples'] = int(re.search(r'(\d+)', line).group(1))
            break

    kappa_section = {}
    for i, line in enumerate(lines):
        if 'Mean Îº:' in line:
            kappa_section['mean'] = float(re.search(r'([\d.]+)', line).group(1))
        elif 'Range:' in line and 'Îº' in lines[i-2]:
            match = re.search(r'\[([-\d.]+), ([\d.]+)\]', line)
            kappa_section['range'] = [float(match.group(1)), float(match.group(2))]

    agreement_dist = {}
    agreement_pct = {}
    for line in lines:
        match = re.match(r'\s+(poor|moderate|substantial|almost_perfect):\s+(\d+)\s+\(([\d.]+)%\)', line)
        if match:
            level = match.group(1)
            count = int(match.group(2))
            pct = float(match.group(3))
            agreement_dist[level] = count
            agreement_pct[level] = pct

    kappa_section['distribution'] = agreement_dist
    kappa_section['percentages'] = agreement_pct
    data['kappa'] = kappa_section

    conf_section = {}
    for i, line in enumerate(lines):
        if 'Mean confidence:' in line:
            conf_section['mean'] = float(re.search(r'([\d.]+)', line).group(1))
        elif 'Range:' in line and 'confidence' in lines[i-2].lower():
            match = re.search(r'\[([\d.]+), ([\d.]+)\]', line)
            conf_section['range'] = [float(match.group(1)), float(match.group(2))]

    conf_dist = {}
    conf_pct = {}
    for line in lines:
        match = re.match(r'\s+(low|medium|high):\s+(\d+)\s+\(([\d.]+)%\)', line)
        if match:
            level = match.group(1)
            count = int(match.group(2))
            pct = float(match.group(3))
            conf_dist[level] = count
            conf_pct[level] = pct

    conf_section['distribution'] = conf_dist
    conf_section['percentages'] = conf_pct
    data['confidence'] = conf_section

    quality_tiers = {}
    for line in lines:
        if 'High quality:' in line:
            match = re.search(r'(\d+)\s+\(([\d.]+)%\)', line)
            if match:
                quality_tiers['high'] = (int(match.group(1)), float(match.group(2)))
        elif 'Medium quality:' in line:
            match = re.search(r'(\d+)\s+\(([\d.]+)%\)', line)
            if match:
                quality_tiers['medium'] = (int(match.group(1)), float(match.group(2)))
        elif 'Low quality:' in line:
            match = re.search(r'(\d+)\s+\(([\d.]+)%\)', line)
            if match:
                quality_tiers['low'] = (int(match.group(1)), float(match.group(2)))
        elif 'Needs review:' in line:
            match = re.search(r'(\d+)\s+\(([\d.]+)%\)', line)
            if match:
                quality_tiers['needs_review'] = (int(match.group(1)), float(match.group(2)))

    data['quality_tiers'] = quality_tiers

    consensus = {}
    for line in lines:
        if 'Overall average:' in line:
            match = re.search(r'([\d.]+)\s+\[([\d.]+),\s+([\d.]+)\]', line)
            if match:
                consensus['overall'] = {
                    'avg': float(match.group(1)),
                    'range': [float(match.group(2)), float(match.group(3))]
                }
        elif 'Factual accuracy:' in line:
            match = re.search(r'([\d.]+)\s+\[([\d.]+),\s+([\d.]+)\]', line)
            if match:
                consensus['factual_accuracy'] = {
                    'avg': float(match.group(1)),
                    'range': [float(match.group(2)), float(match.group(3))]
                }
        elif 'Groundedness:' in line:
            match = re.search(r'([\d.]+)\s+\[([\d.]+),\s+([\d.]+)\]', line)
            if match:
                consensus['groundedness'] = {
                    'avg': float(match.group(1)),
                    'range': [float(match.group(2)), float(match.group(3))]
                }
        elif 'Consistency:' in line:
            match = re.search(r'([\d.]+)\s+\[([\d.]+),\s+([\d.]+)\]', line)
            if match:
                consensus['consistency'] = {
                    'avg': float(match.group(1)),
                    'range': [float(match.group(2)), float(match.group(3))]
                }
        elif 'Completeness:' in line:
            match = re.search(r'([\d.]+)\s+\[([\d.]+),\s+([\d.]+)\]', line)
            if match:
                consensus['completeness'] = {
                    'avg': float(match.group(1)),
                    'range': [float(match.group(2)), float(match.group(3))]
                }

    data['consensus_scores'] = consensus

    model_bias = {}
    current_model = None
    for line in lines:
        stripped = line.strip()
        if stripped.startswith('gpt4o:'):
            current_model = 'gpt4o'
            model_bias[current_model] = {}
        elif stripped.startswith('gpt5.1:'):
            current_model = 'gpt5.1'
            model_bias[current_model] = {}
        elif stripped.startswith('claude_sonnet:'):
            current_model = 'claude_sonnet'
            model_bias[current_model] = {}
        elif stripped.startswith('claude_haiku:'):
            current_model = 'claude_haiku'
            model_bias[current_model] = {}
        elif current_model and 'Average:' in line:
            match = re.search(r'([\d.]+)\s+\[([\d.]+),\s+([\d.]+)\]', line)
            if match:
                model_bias[current_model]['average'] = {
                    'mean': float(match.group(1)),
                    'range': [float(match.group(2)), float(match.group(3))]
                }
        elif current_model and 'Factual accuracy:' in line:
            match = re.search(r'([\d.]+)', line)
            if match:
                model_bias[current_model]['factual_accuracy'] = float(match.group(1))
        elif current_model and 'Groundedness:' in line:
            match = re.search(r'([\d.]+)', line)
            if match:
                model_bias[current_model]['groundedness'] = float(match.group(1))
        elif current_model and 'Consistency:' in line:
            match = re.search(r'([\d.]+)', line)
            if match:
                model_bias[current_model]['consistency'] = float(match.group(1))
        elif current_model and 'Completeness:' in line:
            match = re.search(r'([\d.]+)', line)
            if match:
                model_bias[current_model]['completeness'] = float(match.group(1))

    data['model_bias'] = model_bias

    task_quality = {}
    current_task = None
    for line in lines:
        stripped = line.strip()
        if stripped and stripped.endswith(':') and stripped[:-1] in {'coding', 'dialogue', 'general', 'math', 'qa', 'summarization'}:
            current_task = stripped[:-1]
            task_quality[current_task] = {}
        elif current_task:
            if 'Count:' in line:
                match = re.search(r'(\d+)', line)
                if match:
                    task_quality[current_task]['count'] = int(match.group(1))
            elif 'Mean consensus:' in line:
                match = re.search(r'([\d.]+)', line)
                if match:
                    task_quality[current_task]['consensus'] = float(match.group(1))
            elif 'Mean Îº:' in line:
                match = re.search(r'([\d.]+)', line)
                if match:
                    task_quality[current_task]['kappa'] = float(match.group(1))
            elif 'Mean confidence:' in line:
                match = re.search(r'([\d.]+)', line)
                if match:
                    task_quality[current_task]['confidence'] = float(match.group(1))
            elif 'High quality:' in line:
                match = re.search(r'(\d+)\s+\(([\d.]+)%\)', line)
                if match:
                    task_quality[current_task]['high_quality_pct'] = float(match.group(2))
            elif 'Needs review:' in line:
                match = re.search(r'(\d+)\s+\(([\d.]+)%\)', line)
                if match:
                    task_quality[current_task]['needs_review_pct'] = float(match.group(2))

    data['task_quality'] = task_quality

    return data


def create_agreement_chart_quality(data: dict) -> go.Figure:
    """Create inter-rater agreement distribution chart"""
    kappa_data = data['kappa']
    levels = ['poor', 'moderate', 'substantial', 'almost_perfect']
    colors_map = {
        'poor': QUALITY_COLORS['poor'],
        'moderate': QUALITY_COLORS['moderate'],
        'substantial': QUALITY_COLORS['substantial'],
        'almost_perfect': QUALITY_COLORS['almost_perfect']
    }

    fig = go.Figure()
    for level in levels:
        pct = kappa_data['percentages'].get(level, 0)
        count = kappa_data['distribution'].get(level, 0)
        fig.add_trace(go.Bar(
            name=level.replace('_', ' ').title(),
            x=[pct],
            y=['Distribution'],
            orientation='h',
            marker_color=colors_map[level],
            text=f'{pct:.1f}% ({count})',
            textposition='inside',
            hovertemplate=f'{level.replace("_", " ").title()}: {pct:.1f}% ({count} samples)<extra></extra>'
        ))

    fig.update_layout(
        barmode='stack',
        title='Inter-Rater Agreement',
        xaxis_title='Percentage',
        yaxis_title='',
        height=300,
        showlegend=True,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        margin=dict(l=20, r=20, t=100, b=20)
    )
    return fig


def create_confidence_chart_quality(data: dict) -> go.Figure:
    """Create collective confidence distribution chart"""
    conf_data = data['confidence']
    levels = ['low', 'medium', 'high']
    colors_map = {
        'low': QUALITY_COLORS['confidence_low'],
        'medium': QUALITY_COLORS['confidence_medium'],
        'high': QUALITY_COLORS['confidence_high']
    }

    fig = go.Figure()
    for level in levels:
        pct = conf_data['percentages'].get(level, 0)
        count = conf_data['distribution'].get(level, 0)
        fig.add_trace(go.Bar(
            name=level.title(),
            x=[pct],
            y=['Distribution'],
            orientation='h',
            marker_color=colors_map[level],
            text=f'{pct:.1f}% ({count})',
            textposition='inside',
            hovertemplate=f'{level.title()}: {pct:.1f}% ({count} samples)<extra></extra>'
        ))

    fig.update_layout(
        barmode='stack',
        title='Collective Confidence Distribution',
        xaxis_title='Percentage',
        yaxis_title='',
        height=300,
        showlegend=True,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        margin=dict(l=20, r=20, t=100, b=20)
    )
    return fig


def create_quality_pie_quality(data: dict) -> go.Figure:
    """Create quality tier distribution pie chart"""
    quality_data = data['quality_tiers']
    labels = ['High Quality', 'Medium Quality', 'Low Quality']
    values = [quality_data['high'][1], quality_data['medium'][1], quality_data['low'][1]]
    colors = [QUALITY_COLORS['high'], QUALITY_COLORS['medium'], QUALITY_COLORS['low']]

    fig = go.Figure(data=[go.Pie(
        labels=labels,
        values=values,
        marker_colors=colors,
        hole=0.4,
        textinfo='label+percent',
        hovertemplate='%{label}: %{value:.1f}%<br>(%{text} samples)<extra></extra>',
        text=[quality_data['high'][0], quality_data['medium'][0], quality_data['low'][0]]
    )])

    fig.update_layout(
        title='Quality Tier Distribution',
        annotations=[dict(text=f'{data["total_samples"]}<br>samples', x=0.5, y=0.5, font_size=16, showarrow=False)],
        height=400,
        margin=dict(l=20, r=20, t=60, b=20)
    )
    return fig


def create_consensus_chart_quality(data: dict) -> go.Figure:
    """Create consensus scores bar chart with ranges"""
    consensus = data['consensus_scores']
    dimensions = ['Overall', 'Factual Accuracy', 'Groundedness', 'Consistency', 'Completeness']
    scores = []
    error_minus = []
    error_plus = []

    dim_keys = ['overall', 'factual_accuracy', 'groundedness', 'consistency', 'completeness']
    for key in dim_keys:
        if key in consensus:
            avg = consensus[key]['avg']
            min_val = consensus[key]['range'][0]
            max_val = consensus[key]['range'][1]
            scores.append(avg)
            error_minus.append(avg - min_val)
            error_plus.append(max_val - avg)

    fig = go.Figure()
    fig.add_trace(go.Bar(
        y=dimensions,
        x=scores,
        orientation='h',
        marker_color='#3498db',
        text=[f'{s:.3f}' for s in scores],
        textposition='outside',
        error_x=dict(
            type='data',
            symmetric=False,
            array=error_plus,
            arrayminus=error_minus,
            color='gray'
        ),
        hovertemplate='%{y}: %{x:.3f}<extra></extra>'
    ))

    fig.add_vline(x=4.0, line_dash="dash", line_color="green", annotation_text="4.0 threshold")
    fig.update_layout(
        title='Consensus Scores (with Min-Max Range)',
        xaxis_title='Score (1-5)',
        xaxis=dict(range=[0, 5.5]),
        height=400,
        margin=dict(l=150, r=20, t=60, b=20)
    )
    return fig


def create_model_bias_chart_quality(data: dict) -> go.Figure:
    """Create model bias comparison chart"""
    model_bias = data['model_bias']
    models = ['gpt4o', 'gpt5.1', 'claude_sonnet', 'claude_haiku']
    dimensions = ['average', 'factual_accuracy', 'groundedness', 'consistency', 'completeness']
    dim_labels = ['Average', 'Factual', 'Groundedness', 'Consistency', 'Completeness']

    fig = go.Figure()
    for model in models:
        if model in model_bias:
            values = []
            for dim in dimensions:
                if dim == 'average':
                    values.append(model_bias[model]['average']['mean'])
                else:
                    values.append(model_bias[model].get(dim, 0))

            fig.add_trace(go.Bar(
                name=model.replace('_', ' ').title(),
                x=dim_labels,
                y=values,
                marker_color=QUALITY_COLORS.get(model, '#95a5a6'),
                text=[f'{v:.2f}' for v in values],
                textposition='outside'
            ))

    fig.update_layout(
        title='Model Bias Comparison',
        xaxis_title='Dimension',
        yaxis_title='Score',
        yaxis=dict(range=[0, 5.5]),
        barmode='group',
        height=500,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        margin=dict(l=20, r=20, t=80, b=20)
    )
    return fig


def create_task_table_quality(data: dict) -> pd.DataFrame:
    """Create styled task quality comparison table"""
    task_quality = data['task_quality']
    rows = []
    for task, metrics in task_quality.items():
        rows.append({
            'Task': task.capitalize(),
            'Count': metrics.get('count', 0),
            'Consensus': f"{metrics.get('consensus', 0):.3f}",
            'Kappa (Îº)': f"{metrics.get('kappa', 0):.3f}",
            'Confidence': f"{metrics.get('confidence', 0):.3f}",
            'High Quality %': f"{metrics.get('high_quality_pct', 0):.1f}%",
            'Needs Review %': f"{metrics.get('needs_review_pct', 0):.1f}%"
        })
    return pd.DataFrame(rows)


def render_benchmark_quality_page() -> None:
    st.title("ğŸ“Š Benchmark Dataset Quality Evaluation")
    st.markdown("### Cross-Model Validation by 4 Judge LLMs")
    st.markdown("---")

    project_root = Path(__file__).resolve().parents[1]
    data_file = project_root / "benchmark" / "judge_llm_result.txt"
    if not data_file.exists():
        st.error(f"âŒ Data file not found: {data_file}")
        st.info("Please ensure benchmark/judge_llm_result.txt exists")
        return

    data = parse_judge_results_quality(str(data_file))

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric(label="ğŸ“ˆ Total Samples", value=data['total_samples'])

    with col2:
        st.metric(
            label="ğŸ¯ Mean Kappa (Îº)",
            value=f"{data['kappa']['mean']:.3f}",
            delta="Moderate Agreement"
        )

    with col3:
        st.metric(
            label="âœ… Mean Confidence",
            value=f"{data['confidence']['mean']:.3f}",
            delta="High"
        )

    with col4:
        high_pct = data['quality_tiers']['high'][1]
        st.metric(
            label="ğŸ† High Quality",
            value=f"{high_pct:.1f}%",
            delta=f"{data['quality_tiers']['high'][0]} samples"
        )

    st.markdown("---")

    col1, col2 = st.columns(2)

    with col1:
        fig1 = create_agreement_chart_quality(data)
        st.plotly_chart(fig1, use_container_width=True)

    with col2:
        fig2 = create_confidence_chart_quality(data)
        st.plotly_chart(fig2, use_container_width=True)

    st.markdown("---")

    col1, col2 = st.columns(2)

    with col1:
        with st.expander("â„¹ï¸ Quality Tier ê³„ì‚° ë°©ë²•", expanded=False):
            st.markdown("""
**4ê°€ì§€ ì§€í‘œë¥¼ ì¢…í•© í‰ê°€:**
- Overall Average â‰¥ 3.5
- Inter-Rater Aggrement  â‰¥ 0.5
- Collective Confidence â‰¥ 0.75
- Factual Accuracy â‰¥ 3.5

â†’ **High** / **Medium** / **Low** 3ë‹¨ê³„ ë¶„ë¥˜
            """)
        fig3 = create_quality_pie_quality(data)
        st.plotly_chart(fig3, use_container_width=True)

    with col2:
        with st.expander("â„¹ï¸ Consensus Scores ê³„ì‚° ë°©ë²•", expanded=False):
            st.markdown("""
**4ê°œ Judge LLM ì ìˆ˜ì˜ ë‹¨ìˆœ í‰ê· :**

ê° ì°¨ì›ë³„ í‰ê·  ê³„ì‚°:
- Factual Accuracy
- Groundedness
- Consistency
- Completeness

â†’ 4ê°œ ì°¨ì› í‰ê· ì˜ í‰ê·  = **Overall**
            """)
        fig4 = create_consensus_chart_quality(data)
        st.plotly_chart(fig4, use_container_width=True)

    st.markdown("---")

    st.markdown("### Model Bias Comparison")
    with st.expander("â„¹ï¸ Model Bias ë¶„ì„ ë°©ë²•", expanded=False):
        st.markdown("""
**ê° ëª¨ë¸ì˜ í‰ê·  ì ìˆ˜ ë¹„êµ:**

4ê°œ Judge LLMì´ 244ê°œ ìƒ˜í”Œì— ëŒ€í•´ ë§¤ê¸´ ì ìˆ˜ì˜ í‰ê· ì„ ë¹„êµí•˜ì—¬:
- ì–´ëŠ ëª¨ë¸ì´ ë” ì—„ê²©í•˜ê²Œ í‰ê°€í•˜ëŠ”ì§€ (ë‚®ì€ ì ìˆ˜)
- ì–´ëŠ ëª¨ë¸ì´ ë” ê´€ëŒ€í•˜ê²Œ í‰ê°€í•˜ëŠ”ì§€ (ë†’ì€ ì ìˆ˜)
- ëª¨ë¸ ê°„ ì ìˆ˜ ì°¨ì´ê°€ í° ì°¨ì›ì€ ë¬´ì—‡ì¸ì§€

ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.

**ì˜ë¯¸**: ê° Judge LLMì˜ í‰ê°€ ì„±í–¥ ì°¨ì´
        """)
    fig5 = create_model_bias_chart_quality(data)
    st.plotly_chart(fig5, use_container_width=True)

    st.markdown("---")

    st.markdown("### Quality by Task Type")
    df = create_task_table_quality(data)
    st.dataframe(df, use_container_width=True, hide_index=True)

    st.markdown("---")

    with st.expander("ğŸ” ì£¼ìš” ë°œê²¬ ì‚¬í•­", expanded=False):
        st.markdown("**í‰ê°€ì ê°„ ì¼ì¹˜ë„:**")
        st.markdown(f"- í‰ê·  Inter-Rater Aggrement: **{data['kappa']['mean']:.3f}** (ì¤‘ê°„ ìˆ˜ì¤€ ì¼ì¹˜ë„)")
        st.markdown(f"- {data['kappa']['percentages'].get('almost_perfect', 0):.1f}%ì˜ ìƒ˜í”Œì´ ê±°ì˜ ì™„ë²½í•œ ì¼ì¹˜ë„ë¥¼ ë³´ì„ (Îº â‰¥ 0.8)")
        st.markdown(f"- {data['kappa']['percentages'].get('poor', 0):.1f}%ì˜ ìƒ˜í”Œì´ ë‚®ì€ ì¼ì¹˜ë„ë¥¼ ë³´ì„ (Îº < 0.4)")

        st.markdown("\n**ì‹ ë¢°ë„:**")
        st.markdown(f"- í‰ê·  ì§‘ë‹¨ ì‹ ë¢°ë„: **{data['confidence']['mean']:.3f}**")
        st.markdown(f"- {data['confidence']['percentages'].get('high', 0):.1f}%ì˜ ìƒ˜í”Œì´ ë†’ì€ ì‹ ë¢°ë„ë¥¼ ê°€ì§ (â‰¥ 0.85)")
        st.markdown(f"- {data['confidence']['percentages'].get('low', 0):.1f}%ì˜ ìƒ˜í”Œì´ ë‚®ì€ ì‹ ë¢°ë„ë¥¼ ê°€ì§ (< 0.7)")

        st.markdown("\n**í’ˆì§ˆ ë“±ê¸‰:**")
        st.markdown(f"- {data['quality_tiers']['high'][1]:.1f}%ì˜ ìƒ˜í”Œì´ ê³ í’ˆì§ˆ ({data['quality_tiers']['high'][0]}ê°œ ìƒ˜í”Œ)")
        st.markdown(f"- {data['quality_tiers']['needs_review'][1]:.1f}%ì˜ ìƒ˜í”Œì´ ì¬ê²€í†  í•„ìš” ({data['quality_tiers']['needs_review'][0]}ê°œ ìƒ˜í”Œ)")

        st.markdown("\n**íƒœìŠ¤í¬ë³„ ì¸ì‚¬ì´íŠ¸:**")
        task_quality = data['task_quality']
        best_task = max(task_quality.items(), key=lambda x: x[1].get('kappa', 0))
        worst_task = min(task_quality.items(), key=lambda x: x[1].get('kappa', 0))
        st.markdown(f"- ê°€ì¥ ë†’ì€ ì¼ì¹˜ë„: **{best_task[0].capitalize()}** (Îº = {best_task[1]['kappa']:.3f})")
        st.markdown(f"- ê°€ì¥ ì–´ë ¤ìš´ íƒœìŠ¤í¬: **{worst_task[0].capitalize()}** (Îº = {worst_task[1]['kappa']:.3f}, ì¬ê²€í†  í•„ìš” {worst_task[1].get('needs_review_pct', 0):.1f}%)")

    with st.expander("ğŸ“ ì§€í‘œ ê³„ì‚° ë°©ë²•", expanded=False):
        st.markdown("### Consensus Scores ê³„ì‚°")
        st.markdown("""
**ê³„ì‚° ë°©ì‹**: 4ê°œ Judge LLM(GPT-4o, GPT-5.1, Claude Sonnet, Claude Haiku)ì´ ë§¤ê¸´ ì ìˆ˜ì˜ ë‹¨ìˆœ í‰ê· 

**ë‹¨ê³„**:
1. ê° ì°¨ì›(Factual Accuracy, Groundedness, Consistency, Completeness)ë³„ë¡œ 4ê°œ ëª¨ë¸ì˜ ì ìˆ˜ë¥¼ í‰ê· 
2. 4ê°œ ì°¨ì› í‰ê· ì˜ í‰ê· ì„ Overall Averageë¡œ ê³„ì‚°

**ì˜ˆì‹œ**:
```
Factual Accuracy: (4.8 + 4.5 + 4.3 + 4.6) / 4 = 4.55
Groundedness: (4.8 + 4.5 + 4.5 + 4.5) / 4 = 4.56
Consistency: (5.0 + 4.9 + 4.6 + 4.9) / 4 = 4.83
Completeness: (4.8 + 4.4 + 4.2 + 4.5) / 4 = 4.44
â†’ Overall: (4.55 + 4.56 + 4.83 + 4.44) / 4 = 4.596
```

**ì˜ë¯¸**: ëª¨ë¸ë“¤ì´ ì´ ìƒ˜í”Œì„ í‰ê· ì ìœ¼ë¡œ ëª‡ ì ì´ë¼ê³  í‰ê°€í–ˆëŠ”ê°€?
        """)

        st.markdown("---")

        st.markdown("### Quality Tier ë¶„ë¥˜")
        st.markdown("""
**ê³„ì‚° ë°©ì‹**: 4ê°€ì§€ ì§€í‘œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ High/Medium/Lowë¡œ ë¶„ë¥˜

**ë¶„ë¥˜ ê¸°ì¤€**:

**ğŸŸ¢ High Quality**:
- Overall Average â‰¥ 3.5 (ì „ë°˜ì ìœ¼ë¡œ ì¤€ìˆ˜í•œ í’ˆì§ˆ)
- Inter-Rater Aggrement â‰¥ 0.5 (moderate~substantial ìˆ˜ì¤€ í•©ì˜)
- Collective Confidence â‰¥ 0.75 (ëª¨ë¸ ê°„ ì˜ê²¬ì´ ê½¤ ì¼ê´€ë¨)
- Factual Accuracy â‰¥ 3.5 (ì‚¬ì‹¤ì„±ë„ ì¶©ë¶„íˆ ë†’ìŒ)

**ğŸŸ¡ Medium Quality**:
- Overall Average â‰¥ 3.0
- Inter-Rater Aggrement â‰¥ 0.3
- Collective Confidence â‰¥ 0.6

**ğŸ”´ Low Quality**:
- ìœ„ ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ëª»í•¨

**ì˜ˆì‹œ**:
```
ìƒ˜í”Œ A: Overall 4.5, Îº=0.7, Conf=0.85, Factual=4.6 â†’ High âœ…
ìƒ˜í”Œ B: Overall 3.2, Îº=0.35, Conf=0.65, Factual=3.1 â†’ Medium âš ï¸
ìƒ˜í”Œ C: Overall 2.8, Îº=0.2, Conf=0.5, Factual=2.5 â†’ Low âŒ
```

**ì˜ë¯¸**: ì´ ìƒ˜í”Œì´ ë²¤ì¹˜ë§ˆí¬ë¡œ ì‚¬ìš©í•˜ê¸°ì— ì–¼ë§ˆë‚˜ ì‹ ë¢°í•  ë§Œí•œê°€?
        """)

        st.markdown("---")

        st.markdown("### Fleiss' Kappa (Inter-Rater Agreement)")
        st.markdown("""
**ê³„ì‚° ë°©ì‹**: Fleiss' Kappaë¥¼ ì‚¬ìš©í•˜ì—¬ 4ê°œ ëª¨ë¸ì˜ ì¼ì¹˜ë„ ì¸¡ì •

**ê³µì‹**:
```
Îº = (P_o - P_e) / (1 - P_e)

P_o: ê´€ì°°ëœ ì¼ì¹˜ë„
P_e: ìš°ì—°ì— ì˜í•œ ê¸°ëŒ€ ì¼ì¹˜ë„ (1/5, 1-5 ì ìˆ˜ ë²”ìœ„)
```

**ì˜ˆì‹œ**:
```
ì ìˆ˜ [4, 4, 4, 3]
â†’ 4ì ì— 3ëª… ì¼ì¹˜, 3ì ì— 1ëª…
â†’ P_o = (3Ã—2 + 1Ã—0) / (4Ã—3) = 0.5
â†’ Îº = (0.5 - 0.2) / (1 - 0.2) = 0.375
```

**í•´ì„ ê¸°ì¤€** (Landis & Koch):
- â‰¥ 0.8: Almost Perfect
- 0.6-0.8: Substantial
- 0.4-0.6: Moderate
- < 0.4: Poor

**ì˜ë¯¸**: ëª¨ë¸ë“¤ì´ ê°™ì€ ì ìˆ˜ë¥¼ ì–¼ë§ˆë‚˜ ìì£¼ ë§¤ê¸°ëŠ”ê°€?
        """)

        st.markdown("---")

        st.markdown("### Collective Confidence")
        st.markdown("""
**ê³„ì‚° ë°©ì‹**: ëª¨ë¸ ì ìˆ˜ì˜ í‘œì¤€í¸ì°¨ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ì˜ ê°•ë„ ì¸¡ì •

**ê³µì‹**:
```
Confidence = 1 - (std_dev / max_std_dev)

std_dev: ëª¨ë¸ ì ìˆ˜ë“¤ì˜ í‘œì¤€í¸ì°¨
max_std_dev: ìµœëŒ€ í‘œì¤€í¸ì°¨ (2.0, 1-5 ì ìˆ˜ ë²”ìœ„)
```

**ì˜ˆì‹œ**:
```
ì ìˆ˜ [4, 4, 4, 4] â†’ std=0.0 â†’ confidence=1.0 (ì™„ì „ í•©ì˜) âœ…
ì ìˆ˜ [4, 4, 4, 3] â†’ std=0.43 â†’ confidence=0.78 (ë†’ì€ í•©ì˜)
ì ìˆ˜ [5, 4, 3, 2] â†’ std=1.12 â†’ confidence=0.44 (ë‚®ì€ í•©ì˜) âš ï¸
```

**í•´ì„ ê¸°ì¤€**:
- â‰¥ 0.85: High
- 0.7-0.85: Medium
- < 0.7: Low

**ì˜ë¯¸**: ëª¨ë¸ë“¤ì˜ ì ìˆ˜ê°€ ë¹„ìŠ·í•œ ë²”ìœ„ì— ìˆëŠ”ê°€?
        """)


def main() -> None:
    ensure_page_config()
    benchmark_info = get_benchmark_info()
    render_sidebar(benchmark_info)
    render_benchmark_quality_page()


if __name__ == "__main__":
    main()
